version: '3.8'

services:
  # Backend API Service
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: clarifymeet-backend
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - OLLAMA_MODEL=tinyllama
      - SESSION_TTL_MINUTES=60
      - CORS_ORIGINS=["http://localhost:8080","http://127.0.0.1:8080"]
    volumes:
      - ./backend:/app
      - /app/__pycache__
    restart: unless-stopped
    networks:
      - clarifymeet-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Frontend Service (Simple HTTP Server)
  frontend:
    image: python:3.11-slim
    container_name: clarifymeet-frontend
    ports:
      - "8080:8080"
    volumes:
      - ./frontend:/app
    working_dir: /app
    command: python -m http.server 8080
    restart: unless-stopped
    networks:
      - clarifymeet-network
    depends_on:
      - backend

networks:
  clarifymeet-network:
    driver: bridge

# Note: Ollama must be running on the host machine
# Install Ollama: curl -fsSL https://ollama.com/install.sh | sh
# Pull model: ollama pull tinyllama
# Start Ollama: ollama serve (runs on http://localhost:11434)
